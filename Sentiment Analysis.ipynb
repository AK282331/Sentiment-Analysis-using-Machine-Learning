{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc251a80-1c4d-43eb-a168-7c4172507f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis is the technique to understand the sentiment behind the text. In this assignment sentiment analysis has been done on the reviews\n",
    "#given by the students on movies or products. The data was collected through a google form. \n",
    "\n",
    "#Form Link - https://docs.google.com/forms/d/1Xnu_T5FRgxBFBJAucqqsZKTctgB71grbo3nFy3XKwqs/edit\n",
    "#Response Link - https://docs.google.com/spreadsheets/d/1LnKBY_wucqR1MjdmnWLaZV7yl8OwwmypuYES8KsYa-w/edit?gid=1848984344#gid=1848984344\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "608f4e2e-aefc-4b41-907e-234fa39d6318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhinav\n",
      "[nltk_data]     Khandelwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "import re\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBRFClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4ee898d0-b82e-4248-9ee0-a4b60612a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Abhinav Khandelwal\\Desktop\\Machine Learning\\Sentiment Analysis using Machine Learning\\Sentiment Data  - Form Responses 2.csv\", usecols = [\"Mention your comment here it can be related to the movie or any product.\",\n",
    "                                                                                                                           \"Sentiment of the above comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5b1e2824-08b8-41ad-bb5c-c16e46e49a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"Reviews\",\"Sentiments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "92adde2b-932d-4819-9d66-2160fb0d4fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recently I have purchaced a shoe from decathel...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L2: Empuraan \\nits a good movie to watch .</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Absolutely loved it! The storyline kept me hoo...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The acting was phenomenal and the visuals were...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great user experience – smooth and intuitive d...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews Sentiments\n",
       "0  Recently I have purchaced a shoe from decathel...   Positive\n",
       "1         L2: Empuraan \\nits a good movie to watch .   Positive\n",
       "2  Absolutely loved it! The storyline kept me hoo...   Positive\n",
       "3  The acting was phenomenal and the visuals were...   Positive\n",
       "4  Great user experience – smooth and intuitive d...   Positive"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f444d0f9-a0b6-4a83-bb66-184590724f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiments\n",
       "Positive    74\n",
       "Negative    68\n",
       "Neutral     56\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sentiments\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8576aa6f-2a9b-41e6-b99c-088f0b45942d",
   "metadata": {},
   "source": [
    "# Data Cleaning and Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c097542b-3312-4df7-96b9-6b4957a79d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    clean = text.lower()\n",
    "    reg = re.sub(r\"[!\\n@#$%^&*()-_+=[\\]{}|;:,.<>?/\\'`~\\.]\",\" \",clean)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "901abdde-ccc0-4e95-b1b2-f14fdae1fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Reviews\"] = df[\"Reviews\"].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1a5622cf-f209-43e7-b9fa-073faf62ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    txt = []\n",
    "    for word in text.split():\n",
    "        if word not in stopwords.words(\"english\"):\n",
    "            txt.append(word)\n",
    "        \n",
    "            \n",
    "    e = txt[:]\n",
    "    txt.clear()\n",
    "    return e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ae99f053-d691-408c-ae1a-55c95bb17a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Reviews\"] = df[\"Reviews\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2ec2e538-dc94-4494-88b4-a044223ac353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(text):\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e87df422-a78d-4f03-a0e3-2331385f331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Reviews\"] = df[\"Reviews\"].apply(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ccd3c8c1-57b7-4b01-b713-2618a1542164",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df[\"Reviews\"], df[\"Sentiments\"], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c995d1bc-cc84-46a5-bfda-90242d254e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "32d504d3-7ae2-4e70-a8d0-28d98e46afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b5b5e285-03ff-4cd7-8c29-51b02d33922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vector = cv.fit_transform(x_train).toarray()\n",
    "x_test_vector = cv.transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f9e645bb-3695-4dae-97b0-433a0e0cd03d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutely loved</th>\n",
       "      <th>absolutely loved storyline</th>\n",
       "      <th>absolutely loved storyline kept</th>\n",
       "      <th>acting</th>\n",
       "      <th>acting especially</th>\n",
       "      <th>acting especially lead</th>\n",
       "      <th>acting especially lead actor</th>\n",
       "      <th>acting mediocre</th>\n",
       "      <th>acting mediocre story</th>\n",
       "      <th>...</th>\n",
       "      <th>works exactly described minor</th>\n",
       "      <th>worth</th>\n",
       "      <th>worth money</th>\n",
       "      <th>would</th>\n",
       "      <th>would definitely</th>\n",
       "      <th>would definitely buy</th>\n",
       "      <th>would definitely buy brand</th>\n",
       "      <th>would suggest</th>\n",
       "      <th>would suggest anyone</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 1275 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     absolutely  absolutely loved  absolutely loved storyline  \\\n",
       "0             0                 0                           0   \n",
       "1             0                 0                           0   \n",
       "2             0                 0                           0   \n",
       "3             0                 0                           0   \n",
       "4             0                 0                           0   \n",
       "..          ...               ...                         ...   \n",
       "133           0                 0                           0   \n",
       "134           0                 0                           0   \n",
       "135           0                 0                           0   \n",
       "136           0                 0                           0   \n",
       "137           0                 0                           0   \n",
       "\n",
       "     absolutely loved storyline kept  acting  acting especially  \\\n",
       "0                                  0       0                  0   \n",
       "1                                  0       0                  0   \n",
       "2                                  0       0                  0   \n",
       "3                                  0       0                  0   \n",
       "4                                  0       0                  0   \n",
       "..                               ...     ...                ...   \n",
       "133                                0       0                  0   \n",
       "134                                0       0                  0   \n",
       "135                                0       0                  0   \n",
       "136                                0       0                  0   \n",
       "137                                0       0                  0   \n",
       "\n",
       "     acting especially lead  acting especially lead actor  acting mediocre  \\\n",
       "0                         0                             0                0   \n",
       "1                         0                             0                0   \n",
       "2                         0                             0                0   \n",
       "3                         0                             0                0   \n",
       "4                         0                             0                0   \n",
       "..                      ...                           ...              ...   \n",
       "133                       0                             0                0   \n",
       "134                       0                             0                0   \n",
       "135                       0                             0                0   \n",
       "136                       0                             0                0   \n",
       "137                       0                             0                0   \n",
       "\n",
       "     acting mediocre story  ...  works exactly described minor  worth  \\\n",
       "0                        0  ...                              0      0   \n",
       "1                        0  ...                              0      0   \n",
       "2                        0  ...                              0      0   \n",
       "3                        0  ...                              0      0   \n",
       "4                        0  ...                              0      0   \n",
       "..                     ...  ...                            ...    ...   \n",
       "133                      0  ...                              0      0   \n",
       "134                      0  ...                              0      0   \n",
       "135                      0  ...                              0      0   \n",
       "136                      0  ...                              0      0   \n",
       "137                      0  ...                              0      0   \n",
       "\n",
       "     worth money  would  would definitely  would definitely buy  \\\n",
       "0              0      0                 0                     0   \n",
       "1              0      0                 0                     0   \n",
       "2              0      0                 0                     0   \n",
       "3              0      0                 0                     0   \n",
       "4              0      0                 0                     0   \n",
       "..           ...    ...               ...                   ...   \n",
       "133            0      0                 0                     0   \n",
       "134            0      0                 0                     0   \n",
       "135            0      0                 0                     0   \n",
       "136            0      0                 0                     0   \n",
       "137            0      0                 0                     0   \n",
       "\n",
       "     would definitely buy brand  would suggest  would suggest anyone  year  \n",
       "0                             0              0                     0     0  \n",
       "1                             0              0                     0     0  \n",
       "2                             0              0                     0     0  \n",
       "3                             0              0                     0     0  \n",
       "4                             0              0                     0     0  \n",
       "..                          ...            ...                   ...   ...  \n",
       "133                           0              0                     0     0  \n",
       "134                           0              0                     0     0  \n",
       "135                           0              0                     0     0  \n",
       "136                           0              0                     0     0  \n",
       "137                           0              0                     0     0  \n",
       "\n",
       "[138 rows x 1275 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data = x_train_vector, columns = cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f2f66d25-b114-4485-97eb-9f4049d5e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Over Sampling due to Imbalance Data\n",
    "over = SMOTE(k_neighbors=5)\n",
    "x_train_o, y_train_o = over.fit_resample(x_train_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fa1d7022-5b90-4d65-92e3-04218b5acf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying PCA Feature Decomposition\n",
    "pca = PCA(n_components=90,n_oversamples=50)\n",
    "x_train_new = pca.fit_transform(x_train_o)\n",
    "x_test_new = pca.transform(x_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "36367ff4-86a1-44aa-bd97-8b378a7eb737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.4166666666666667, -- Decision Tree\n",
      "Accuracy Score = 0.4, -- Support Vector\n",
      "Accuracy Score = 0.4166666666666667, -- Random Forest\n",
      "Accuracy Score = 0.45, -- Gradient\n",
      "Accuracy Score = 0.43333333333333335, -- XGBoost\n"
     ]
    }
   ],
   "source": [
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = SVC()\n",
    "clf3 = RandomForestClassifier()\n",
    "clf4 = GradientBoostingClassifier()\n",
    "clf5 = XGBRFClassifier()\n",
    "\n",
    "names = [\"Decision Tree\", \"Support Vector\", \"Random Forest\", \"Gradient\",\"XGBoost\"]\n",
    "\n",
    "for clf, name in zip([clf1,clf2,clf3,clf4,clf5],names):\n",
    "    model = clf.fit(x_train_o,y_train_o)\n",
    "    y_pred = model.predict(x_test_vector)\n",
    "    print(f\"Accuracy Score = {accuracy_score(y_test, y_pred)}, -- {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51c25a-9f54-4f48-beb5-bb70230c934c",
   "metadata": {},
   "source": [
    "# Optuna Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4e51d1ae-4d66-4e9f-a61b-3c6a159bfff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    n_estimators= trial.suggest_int(\"n_estimators\", 62, 70)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 7, 12)\n",
    "    learning_rate =  trial.suggest_float(\"learning_rate\", 0.005, 0.008, log=True)\n",
    "    model = XGBRFClassifier(n_estimators =  n_estimators,max_depth =  max_depth, learning_rate=learning_rate)\n",
    "    model.fit(x_train_o, y_train_o)\n",
    "    y_pred = model.predict(x_test_vector)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e57f6304-81bb-42d5-8587-5d211bd7021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 08:51:52,155] A new study created in memory with name: no-name-074f0f75-98a5-4763-9baa-d9ba92f05c50\n",
      "[I 2025-06-07 08:51:53,802] Trial 0 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 69, 'max_depth': 10, 'learning_rate': 0.006414796407310343}. Best is trial 0 with value: 0.5333333333333333.\n",
      "[I 2025-06-07 08:51:55,480] Trial 1 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 70, 'max_depth': 10, 'learning_rate': 0.007114383796294974}. Best is trial 0 with value: 0.5333333333333333.\n",
      "[I 2025-06-07 08:51:56,678] Trial 2 finished with value: 0.5 and parameters: {'n_estimators': 68, 'max_depth': 7, 'learning_rate': 0.005066909500674922}. Best is trial 0 with value: 0.5333333333333333.\n",
      "[I 2025-06-07 08:51:58,452] Trial 3 finished with value: 0.5166666666666667 and parameters: {'n_estimators': 70, 'max_depth': 11, 'learning_rate': 0.007728254679603975}. Best is trial 0 with value: 0.5333333333333333.\n",
      "[I 2025-06-07 08:52:00,109] Trial 4 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 70, 'max_depth': 10, 'learning_rate': 0.007464299051157142}. Best is trial 0 with value: 0.5333333333333333.\n",
      "[I 2025-06-07 08:52:01,231] Trial 5 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 64, 'max_depth': 7, 'learning_rate': 0.007903886056609086}. Best is trial 0 with value: 0.5333333333333333.\n",
      "[I 2025-06-07 08:52:03,047] Trial 6 finished with value: 0.5166666666666667 and parameters: {'n_estimators': 66, 'max_depth': 12, 'learning_rate': 0.006296262810834991}. Best is trial 0 with value: 0.5333333333333333.\n",
      "[I 2025-06-07 08:52:04,692] Trial 7 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 65, 'max_depth': 11, 'learning_rate': 0.007193084074748799}. Best is trial 0 with value: 0.5333333333333333.\n",
      "[I 2025-06-07 08:52:06,006] Trial 8 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 67, 'max_depth': 8, 'learning_rate': 0.006402395709115117}. Best is trial 0 with value: 0.5333333333333333.\n",
      "[I 2025-06-07 08:52:07,784] Trial 9 finished with value: 0.5166666666666667 and parameters: {'n_estimators': 65, 'max_depth': 12, 'learning_rate': 0.007470647367832745}. Best is trial 0 with value: 0.5333333333333333.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eced81f8-3df2-4922-bc91-806ebc5c47d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBRFClassifier(n_estimators =  69, max_depth =  10, learning_rate = 0.006414796407310343)\n",
    "clf.fit(x_train_o,y_train_o)\n",
    "pred = clf.predict(x_test_vector)\n",
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ef3db-ede7-4a59-96b7-1a9f5a502868",
   "metadata": {},
   "source": [
    "# Taking new Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3da5ad0a-87e3-498e-98f3-8c93bc3d13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = FunctionTransformer(cleaning)\n",
    "remove_stop = FunctionTransformer(remove_stopwords)\n",
    "join = FunctionTransformer(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cbf574df-0739-43cd-8bac-f4c478e3b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_review = \"This movie is very good and we should watch it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "26fe039a-bd6e-42ee-b46c-4678da2d2e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment\n"
     ]
    }
   ],
   "source": [
    "pip_clean = Pipeline(\n",
    "    [\n",
    "    (\"clean\",clean),\n",
    "    (\"stop\",remove_stop),\n",
    "    (\"join\",join),\n",
    "    ]\n",
    ")\n",
    "f_text = pip_clean.transform(new_review)\n",
    "\n",
    "cv_text = cv.transform([f_text]).toarray()\n",
    "pr = clf.predict(cv_text)\n",
    "if(pr == 0):\n",
    "    print(\"Negative Sentiment\")\n",
    "if(pr == 1):\n",
    "    print(\"Neutral Sentiment\")\n",
    "elif pr == 2:\n",
    "    print(\"Positive Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec6a77-a755-4155-ab1d-22e55fdb1b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d37d10-0ec6-4f43-b551-068fc85c5fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
